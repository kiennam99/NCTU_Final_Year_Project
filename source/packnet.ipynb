{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intrinsic K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall pytorch3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../packnet-sfm/\")\n",
    "\n",
    "from packnet_sfm.models.model_wrapper import ModelWrapper\n",
    "from packnet_sfm.datasets.augmentations import resize_image, to_tensor\n",
    "from packnet_sfm.utils.horovod import hvd_init, rank, world_size, print0\n",
    "from packnet_sfm.utils.image import load_image\n",
    "from packnet_sfm.utils.config import parse_test_file\n",
    "from packnet_sfm.utils.load import set_debug\n",
    "from packnet_sfm.utils.depth import write_depth, inv2depth, viz_inv_depth\n",
    "from packnet_sfm.utils.logging import pcolor\n",
    "from packnet_sfm.geometry.pose_utils import euler2mat\n",
    "\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from glob import glob\n",
    "# from pytorch3d import transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image(file, ext=('.png', '.jpg',)):\n",
    "    \"\"\"Check if a file is an image with certain extensions\"\"\"\n",
    "    return file.endswith(ext)\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='PackNet-SfM inference of depth maps from images')\n",
    "    parser.add_argument('--checkpoint', type=str, help='Checkpoint (.ckpt)')\n",
    "    parser.add_argument('--input', type=str, help='Input file or folder')\n",
    "    parser.add_argument('--output', type=str, help='Output file or folder')\n",
    "    parser.add_argument('--image_shape', type=int, nargs='+', default=None,\n",
    "                        help='Input and output image shape '\n",
    "                             '(default: checkpoint\\'s config.datasets.augmentation.image_shape)')\n",
    "    parser.add_argument('--half', action=\"store_true\", help='Use half precision (fp16)')\n",
    "    parser.add_argument('--save', type=str, choices=['npz', 'png'], default=None,\n",
    "                        help='Save format (npz or png). Default is None (no depth map is saved).')\n",
    "    args = parser.parse_args()\n",
    "    assert args.checkpoint.endswith('.ckpt'), \\\n",
    "        'You need to provide a .ckpt file as checkpoint'\n",
    "    assert args.image_shape is None or len(args.image_shape) == 2, \\\n",
    "        'You need to provide a 2-dimensional tuple as shape (H,W)'\n",
    "    assert (is_image(args.input) and is_image(args.output)) or \\\n",
    "           (not is_image(args.input) and not is_image(args.input)), \\\n",
    "        'Input and output must both be images or folders'\n",
    "    return args\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_and_save_depth(input_file, output_file, model_wrapper, image_shape, half, save):\n",
    "    \"\"\"\n",
    "    Process a single input file to produce and save visualization\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_file : str\n",
    "        Image file\n",
    "    output_file : str\n",
    "        Output file, or folder where the output will be saved\n",
    "    model_wrapper : nn.Module\n",
    "        Model wrapper used for inference\n",
    "    image_shape : Image shape\n",
    "        Input image shape\n",
    "    half: bool\n",
    "        use half precision (fp16)\n",
    "    save: str\n",
    "        Save format (npz or png)\n",
    "    \"\"\"\n",
    "    if not is_image(output_file):\n",
    "        # If not an image, assume it's a folder and append the input name\n",
    "        os.makedirs(output_file, exist_ok=True)\n",
    "        output_file = os.path.join(output_file, os.path.basename(input_file))\n",
    "\n",
    "    # change to half precision for evaluation if requested\n",
    "    dtype = torch.float16 if half else None\n",
    "\n",
    "    # Load image\n",
    "    image = load_image(input_file)\n",
    "\n",
    "    # Resize and to tensor\n",
    "    image = resize_image(image, image_shape)\n",
    "    image = to_tensor(image).unsqueeze(0)\n",
    "\n",
    "    # Send image to GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        image = image.to('cuda:{}'.format(rank()), dtype=dtype)\n",
    "    \n",
    "    # Depth inference (returns predicted inverse depth)\n",
    "    pred_inv_depth = model_wrapper.depth(image)['inv_depths'][0]\n",
    "\n",
    "    if save == 'npz' or save == 'png':\n",
    "        # Get depth from predicted depth map and save to different formats\n",
    "        filename = '{}.{}'.format(os.path.splitext(output_file)[0], save)\n",
    "        print('Saving {} to {}'.format(\n",
    "            pcolor(input_file, 'cyan', attrs=['bold']),\n",
    "            pcolor(filename, 'magenta', attrs=['bold'])))\n",
    "        write_depth(filename, depth=inv2depth(pred_inv_depth))\n",
    "        # print(inv2depth(pred_inv_depth))\n",
    "        \n",
    "    else:\n",
    "        # Prepare RGB image\n",
    "        rgb = image[0].permute(1, 2, 0).detach().cpu().numpy() * 255\n",
    "        # Prepare inverse depth\n",
    "        viz_pred_inv_depth = viz_inv_depth(pred_inv_depth[0]) * 255\n",
    "        # Concatenate both vertically\n",
    "        # image = np.concatenate([rgb, viz_pred_inv_depth], 0)\n",
    "        image = viz_pred_inv_depth\n",
    "        # Save visualization\n",
    "        print('Saving {} to {}'.format(\n",
    "            pcolor(input_file, 'cyan', attrs=['bold']),\n",
    "            pcolor(output_file, 'magenta', attrs=['bold'])))\n",
    " \n",
    "\n",
    "def get_images(filename, imgshp):\n",
    "    image = load_image(filename)\n",
    "    image = resize_image(image,imgshp)\n",
    "    image = to_tensor(image).unsqueeze(0)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        imgae = image.to(\"cuda:{}\".format(rank()),dtype=None)\n",
    "\n",
    "    return image\n",
    "\n",
    "poses = dict()\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_and_save_pose(input_file, input_refs, model_wrapper,image_shape):\n",
    "    '''\n",
    "    Output Pose\n",
    "    input_file    : input_file\n",
    "    input_refs    : reference frame \n",
    "    model_wrapper : nn module\n",
    "    '''\n",
    "    basename = os.path.basename(input_file)\n",
    "    \n",
    "    image_ref = [get_images(input_ref,image_shape) for input_ref in input_refs]\n",
    "    image = get_images(input_file,image_shape)\n",
    "\n",
    "    # 1st to 2nd img pose\n",
    "    pose = model_wrapper.pose(image,image_ref)[0][0] \n",
    "    angle = pose[3:]\n",
    "    angle = torch.flip(angle,[0])\n",
    "    angle = angle.reshape(1,-1)\n",
    "    rot_matrix = euler2mat(angle)\n",
    "    rot_matrix.reshape(3,3)\n",
    "    translation = pose[:3]\n",
    "\n",
    "    poses[basename] = (rot_matrix,translation)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 3, 4],\n",
       "        [2, 3, 4],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[[1,3,4],\n",
    "                [2,3,4],[3,4,5]]])\n",
    "x.reshape(3,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image to world coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../camviz/\")\n",
    "from packnet_sfm.geometry.camera_utils import construct_K\n",
    "import cv2\n",
    "# from packnet_sfm.geometry.camera import Camera\n",
    "import camviz as cv \n",
    "import numpy as np\n",
    "\n",
    "# depth\n",
    "fx=9.047872e+02\n",
    "fy=9.017079e+02\n",
    "cx=6.946163e+02 \n",
    "cy=2.353088e+02 \n",
    "K = construct_K(fx,fy,cx,cy)\n",
    "\n",
    "np.savetxt(\"../Kitti/3_pose/intrinsic.txt\", K.numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys.path.append(\"../Kitti/\")\n",
    "ckpt = \"../packnet-sfm/PackNet01_HR_velsup_CStoK.ckpt\"\n",
    "imgshp = (384,1280)\n",
    "\n",
    "inputs=\"../Kitti/3\"  \n",
    "depth_path=\"../Kitti/3_depth/\"\n",
    "outputs=\"../Kitti/3_pose/pose.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384, 1280)\n",
      "\u001b[32m### Preparing Model\u001b[0m\n",
      "\u001b[33mModel: SelfSupModel\u001b[0m\n",
      "\u001b[33mDepthNet: PackNet01\u001b[0m\n",
      "\u001b[33mPoseNet: PoseNet\u001b[0m\n",
      "Found 158 files\n"
     ]
    }
   ],
   "source": [
    "# main:\n",
    "\n",
    "hvd_init()\n",
    "config, state_dict = parse_test_file(ckpt)\n",
    "print(config.datasets.augmentation.image_shape)\n",
    "\n",
    "set_debug(config.debug)\n",
    "\n",
    "# Model Wrapper\n",
    "model_wrapper = ModelWrapper(config,load_datasets=False)\n",
    "model_wrapper.load_state_dict(state_dict)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model_wrapper = model_wrapper.to('cuda:{}'.format(rank()),dtype=None)\n",
    "model_wrapper.eval()\n",
    "\n",
    "\n",
    "if os.path.isdir(inputs):\n",
    "    # If input file is a folder, search for image files\n",
    "    files = []\n",
    "    for ext in ['png', 'jpg']:\n",
    "        files.extend(glob((os.path.join(inputs, '*.{}'.format(ext)))))\n",
    "    files.sort()\n",
    "    print('Found {} files'.format(len(files)))\n",
    "else:\n",
    "    # Otherwise, use it as is\n",
    "    files = [inputs]\n",
    "\n",
    "# infer_and_save_pose(\n",
    "#     input_file=files[0],input_refs=input_ref,output_file=outputs,\n",
    "#     model_wrapper=model_wrapper,image_shape=imgshp)\n",
    "\n",
    "\n",
    "for i in range(1,len(files[rank()::world_size()])-1):\n",
    "    input_file = files[i]\n",
    "    input_ref = [files[i-1],files[i+1]]\n",
    "    infer_and_save_pose(\n",
    "        input_file, input_ref, model_wrapper, imgshp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = np.zeros(3)\n",
    "orientation = np.eye(3)\n",
    "\n",
    "for key in sorted(poses.keys()):\n",
    "    rot_matrix, translation = poses[key]\n",
    "    rot_matrix = rot_matrix.reshape(3,3)\n",
    "    orientation = orientation.dot(rot_matrix.tolist())\n",
    "    position += orientation.dot(translation.tolist())\n",
    "    poses[key] = {\"rot\": rot_matrix.tolist(),\n",
    "                  \"trnas\": translation.tolist(),\n",
    "                  \"poses\": [*orientation[0], position[0], \n",
    "                            *orientation[1], position[1],\n",
    "                            *orientation[2], position[2],\n",
    "                            0, 0, 0, 1]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written pose 158 of images to ../Kitti/3_pose/pose.json\n"
     ]
    }
   ],
   "source": [
    "json.dump(poses, open(outputs,\"w\"), sort_keys=True)\n",
    "print(f\"Written pose {len(poses)} of images to {outputs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12a5d5be5ceda941e8a8debdfa3685374991bd9aa53c79d9666b4b4c3868c7af"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
